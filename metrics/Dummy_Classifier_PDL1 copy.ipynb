{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"my_library\")\n",
    "from my_library.config import *\n",
    "from my_library.metrics.cmp_metrics import *\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ArithmeticErrormodels = [\"br3\", \"adsq\", \"pdl1-3\"] \n",
    "outcomes = [\"BEST_RESPONSE_3\", \"HISTOLOGY\", \"PDL1_CATHEGORY\"]\n",
    "eval = \"eval_1_single_fold\"'''\n",
    "'''\n",
    "base_folder = os.path.join(working_directory,\"eval\")\n",
    "subfolder = [name for name in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, name))]\n",
    "\n",
    "results = []\n",
    "\n",
    "eval = \"eval\"\n",
    "\n",
    "models = [\"br3\", \"adsq\", \"pdl1-3\"] \n",
    "outcomes = [\"BEST_RESPONSE_3\", \"HISTOLOGY\", \"PDL1_CATHEGORY\"]\n",
    "\n",
    "for m,o in zip(models, outcomes):\n",
    "    print(m)\n",
    "    print(o)\n",
    "\n",
    "    outcome = o\n",
    "    model = m\n",
    "    root_dir = os.path.join(base_folder, m)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_df = pd.read_csv(working_directory+\"/projects/I3lung-sqadqc-project/annotations.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_dir = f\"{working_directory}/{eval}/train/\" + model\n",
    "\n",
    "train_parquet_paths = []\n",
    "val_parquet_paths = []\n",
    "test_parquet_paths = []\n",
    "all_parquets_paths = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # Check if the folder name contains \"attention\"\n",
    "    if \"attention\" in os.path.basename(dirpath).lower():\n",
    "\n",
    "        # If you want to do something with each file inside those folders\n",
    "        for f in filenames:\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\".parquet\"):\n",
    "                #print(f\"   This is a Parquet file: {file_path}\")\n",
    "                train_parquet_paths.append(file_path)\n",
    "train_parquet_paths.sort()\n",
    "\n",
    "#print(\"Sorted Parquet paths:\")\n",
    "#for path in train_parquet_paths:\n",
    "#    print(path)\n",
    "\n",
    "root_dir = f\"{working_directory}/{eval}/val/\" + model\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # Check if the folder name contains \"attention\"\n",
    "    if \"attention\" in os.path.basename(dirpath).lower():\n",
    "\n",
    "        # If you want to do something with each file inside those folders\n",
    "        for f in filenames:\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\".parquet\"):\n",
    "                #print(f\"   This is a Parquet file: {file_path}\")\n",
    "                val_parquet_paths.append(file_path)\n",
    "val_parquet_paths.sort()\n",
    "\n",
    "\n",
    "root_dir = f\"{working_directory}/{eval}/test-int/\" + model\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # Check if the folder name contains \"attention\"\n",
    "    if \"attention\" in os.path.basename(dirpath).lower():\n",
    "\n",
    "        # If you want to do something with each file inside those folders\n",
    "        for f in filenames:\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\".parquet\"):\n",
    "                #print(f\"   This is a Parquet file: {file_path}\")\n",
    "                test_parquet_paths.append(file_path)\n",
    "test_parquet_paths.sort()\n",
    "\n",
    "root_dir = f\"{working_directory}/{eval}/all/int_folds_{model}\"\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # Check if the folder name contains \"attention\"\n",
    "    if \"attention\" in os.path.basename(dirpath).lower():\n",
    "\n",
    "        # If you want to do something with each file inside those folders\n",
    "        for f in filenames:\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\".parquet\"):\n",
    "                #print(f\"   This is a Parquet file: {file_path}\")\n",
    "                all_parquets_paths.append(file_path)\n",
    "all_parquets_paths.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_parquets_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Dummy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_parquets_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = []\n",
    "numbers = []\n",
    "\n",
    "test_scores = []\n",
    "test_true_y = []\n",
    "test_pred_y =[]\n",
    "\n",
    "test_list_dataframes = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = all_parquets_paths[0]\n",
    "\n",
    "df_predictions = pd.read_parquet(path)\n",
    "df_joined = pd.merge(df_predictions, csv_df, on ='slide', how = 'left')\n",
    "#print(df_joined)\n",
    "distribution = df_joined[[\"slide\", outcome]]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_list = np.ones((distribution.shape[0], 1))\n",
    "y_list = df_joined[outcome].to_numpy()\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(x_list, y_list)\n",
    "\n",
    "y_pred = dummy_clf.predict(x_list)\n",
    "\n",
    "\n",
    "\n",
    "df_joined = pd.merge(df_predictions, csv_df, on ='slide', how = 'left')\n",
    "#print(df_joined)\n",
    "distribution = df_joined[[\"slide\",outcome]]\n",
    "\n",
    "display(distribution)\n",
    "\n",
    "y_true = df_joined[outcome].to_numpy()\n",
    "print(y_true)\n",
    "x_list = np.ones((distribution.shape[0], 1))\n",
    "\n",
    "\n",
    "\n",
    "#print(distribution)\n",
    "y_pred = dummy_clf.predict(x_list)\n",
    "print(y_pred)\n",
    "\n",
    "test_true_y.append(y_true)\n",
    "test_pred_y.append(y_pred)\n",
    "\n",
    "distribution[\"y_pred\"] = y_pred\n",
    "test_list_dataframes.append(distribution)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_list_dataframes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat(test_list_dataframes, ignore_index = True)\n",
    "test_df.rename(columns={test_df.columns[1]: 'y_true'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_5_elements (train_true_y, train_pred_y):\n",
    "    liste =train_true_y\n",
    "    y_trues = []\n",
    "    for i in range(0, len(liste), 5):\n",
    "        unione = [item for sublist in liste[i:i+5] for item in sublist]\n",
    "        y_trues.append(unione)\n",
    "        #print(unione)\n",
    "\n",
    "    liste = train_pred_y\n",
    "    y_preds = []\n",
    "    for i in range(0, len(liste), 5):\n",
    "        unione = [item for sublist in liste[i:i+5] for item in sublist]\n",
    "        y_preds.append(unione)\n",
    "        #print(unione)\n",
    "    return y_trues, y_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(df_input, name): \n",
    "    df = df_input.copy()\n",
    "    # Removing \"Fold\" column\n",
    "    df = df.drop(columns=[\"Dataset\",\"Fold\"])  \n",
    "    mean_values = df.mean()\n",
    "    std_values = df.std()\n",
    "    n = len(df)\n",
    "    ci_values = (2 * std_values) / np.sqrt(n)\n",
    "    result_df = pd.DataFrame({\n",
    "    \"Column\": mean_values.index,\n",
    "    \"Mean\": mean_values.values,\n",
    "    \"CI\": ci_values.values\n",
    "    })\n",
    "    result_df = result_df.add_prefix(name)\n",
    "    display(result_df)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(train_true_y, train_pred_y, model, name_set):\n",
    "    y_trues, y_preds = extract_5_elements(train_true_y, train_pred_y)\n",
    "    results = []  # Store results\n",
    "\n",
    "    for i in range(len(y_trues)):\n",
    "        y_list = y_trues[i]\n",
    "        y_pred = y_preds[i]\n",
    "        acc = accuracy_score(y_list, y_pred)\n",
    "        precision = precision_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "        recall = recall_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "        f1 = f1_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        report = classification_report(y_list, y_pred, digits=2, output_dict=True, zero_division=0)\n",
    "\n",
    "        # Flatten report into a list\n",
    "        row = {\"Dataset\": name_set, \"Fold\": i+1, \"Accuracy\": acc, \"Precision\" : precision, \"Recall\": recall, \"F1_score\" : f1}\n",
    "\n",
    "        for label, metrics in report.items():\n",
    "            if isinstance(metrics, dict):  # Avoid 'accuracy' which is a single value\n",
    "                for metric_name, value in metrics.items():\n",
    "                    row[f\"{label}_{metric_name}\"] = value\n",
    "        \n",
    "        results.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "    display(df_results)\n",
    "    #df_results.to_excel(model+f\"_{name_set}_5_folds.xlsx\")\n",
    "\n",
    "    avg = average(df_results, name_set)\n",
    "\n",
    "    return df_results, avg\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1, avg_1 = extract_metrics( train_true_y, train_pred_y, model, \"train\")\n",
    "#df2, avg_2= extract_metrics( val_true_y, val_pred_y, model, \"val\")\n",
    "df3, avg_3 = extract_metrics( test_true_y, test_pred_y, model, \"test\")\n",
    "\n",
    "#df_final = pd.concat([df1, df2, df3], axis=0, ignore_index=True)\n",
    "#df_final.to_excel(model+f\"_complete_5_folds.xlsx\")\n",
    "\n",
    "#df_final_avg = pd.concat([avg_1, avg_2, avg_3], axis=1, ignore_index=False)\n",
    "#df_final_avg.to_excel(model+f\"_complete_averages_5_folds.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import TestCase\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppiamo i dati per slide e y_true e creiamo nuove colonne per ogni valore di y_pred_max_idx\n",
    "df_final = test_df.groupby([\"slide\", \"y_true\"])[\"y_pred\"].agg(list).reset_index()\n",
    "\n",
    "# Espandiamo la lista in colonne separate\n",
    "df_final_expanded = df_final[\"y_pred\"].apply(pd.Series)\n",
    "\n",
    "# Rinominiamo le nuove colonne\n",
    "df_final_expanded.columns = [f\"pred_{i+1}\" for i in df_final_expanded.columns]\n",
    "\n",
    "# Concatenazione con le colonne originali\n",
    "df_result = pd.concat([df_final.drop(columns=[\"y_pred\"]), df_final_expanded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [col for col in df_result.columns if 'pred_' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supponiamo che il tuo DataFrame si chiami df\n",
    "colonne_pred = [col for col in df_result.columns if 'pred_' in col]#[f\"pred_{i}\" for i in range(1, 6)]\n",
    "\n",
    "# Calcola il valore più frequente (majority vote) per ogni riga\n",
    "df_result['y_pred'] = df_result[colonne_pred].mode(axis=1)[0]\n",
    "\n",
    "# Visualizza il DataFrame aggiornato\n",
    "print(df_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_result[[\"slide\", \"y_true\", \"y_pred\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "def extract_metrics(train_true_y, train_pred_y, model, name_set):\n",
    "    \n",
    "    results = []  # Store results\n",
    "    y_list = train_true_y\n",
    "    y_pred = train_pred_y\n",
    "    acc = accuracy_score(y_list, y_pred)\n",
    "\n",
    "    precision = precision_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "    weighted_accuracy = balanced_accuracy_score(y_list, y_pred)\n",
    "\n",
    "    recall = recall_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    report = classification_report(y_list, y_pred, digits=2,   output_dict=True, zero_division=0)\n",
    "\n",
    "    # Flatten report into a list\n",
    "    row = {\"Dataset\": name_set, \"Fold\": 1, \"Accuracy\": acc, \"Balanced Accuracy\":weighted_accuracy, \"Precision\" : precision, \"Recall\": recall, \"F1_score\" : f1}\n",
    "\n",
    "    for label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Avoid 'accuracy' which is a single value\n",
    "            for metric_name, value in metrics.items():\n",
    "                row[f\"{label}_{metric_name}\"] = value\n",
    "        \n",
    "    results.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "    display(df_results)\n",
    "    #df_results.to_excel(model+f\"_{name_set}_5_folds.xlsx\")\n",
    "\n",
    "    \n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_test[\"y_true\"].to_numpy()\n",
    "y_pred = df_test[\"y_pred\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_metrics(y_true, y_pred, \"br3\", \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
