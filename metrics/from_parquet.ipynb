{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import platform\n",
    "device_name = platform.node()\n",
    "\n",
    "previous_folder = os.getcwd()\n",
    "print (\"This is the working folder: \" + previous_folder)\n",
    "\n",
    "if device_name == 'mmd-MS-7D98': \n",
    "    #This passage is done because everytime I log into the remote server the default folder is:\n",
    "    #/mmd/home and I need to localize the correct folder to load the settings \n",
    "    os.chdir(\"/media/mmd/Samsung_T5/GitHub/UMD\")\n",
    "\n",
    "if previous_folder != os.getcwd(): # This is now the right working folder\n",
    "    print(\"The current working folder has been changed, now the working folder is: \" + os.getcwd())\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"my_library\")\n",
    "from my_library.config import *\n",
    "from my_library.metrics.cmp_metrics import *\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Prediction Files**:\n",
    "   - Walks through directories to find all Parquet files in folders containing \"attention\" in their name\n",
    "   - Collects and sorts paths to these prediction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parquet_paths = []  # Initialize an empty list to store the paths of .parquet files\n",
    "\n",
    "# Recursively walk through all directories and files starting from root_dir\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # Check if the folder name contains the word \"attention\" (case-insensitive)\n",
    "    if \"attention\" in os.path.basename(dirpath).lower():\n",
    "        \n",
    "        # Loop through all files in the current directory\n",
    "        for f in filenames:\n",
    "            file_path = os.path.join(dirpath, f)  # Construct the full file path\n",
    "            \n",
    "            # Check if the file has a .parquet extension (case-insensitive)\n",
    "            if f.lower().endswith(\".parquet\"):\n",
    "                test_parquet_paths.append(file_path)  # Add the file path to the list\n",
    "\n",
    "# Sort the list of file paths alphabetically\n",
    "test_parquet_paths.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  # Import the JSON module to work with JSON files\n",
    "\n",
    "# Replace \"predictions.parquet\" with \"mil_params.json\" in the first path from the list\n",
    "json_p = test_parquet_paths[0].replace(\"predictions.parquet\", \"mil_params.json\")\n",
    "\n",
    "# Open the JSON file and load its contents\n",
    "with open(json_p, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract the values of the 'outcome_labels' dictionary from the loaded data\n",
    "outcome_labels = data['mil_params']['outcome_labels'].values()\n",
    "\n",
    "# Extract the list or dictionary of outcomes\n",
    "outcomes = data['mil_params']['outcomes']\n",
    "\n",
    "# Print the outcome labels\n",
    "print(f'These are the labels: {outcome_labels}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []  # Initialize an empty list to store prediction DataFrames\n",
    "\n",
    "# Loop over each path in the list of test parquet files\n",
    "for i, path in enumerate(test_parquet_paths):\n",
    "    df = pd.read_parquet(path)  # Read the parquet file into a DataFrame\n",
    "\n",
    "    # Select columns that start with \"y_pred\" (predicted class probabilities or scores)\n",
    "    y_pred_columns = [col for col in df.columns if col.startswith(\"y_pred\")]\n",
    "\n",
    "    # For each row, find the column with the maximum value among the y_pred columns\n",
    "    # Extract the class index from the column name (e.g., \"y_pred_2\" → 2)\n",
    "    df[\"y_pred_max_idx\"] = df[y_pred_columns].idxmax(axis=1).str.extract(\"(\\d+)\").astype(int)\n",
    "\n",
    "    # Drop the original y_pred columns to keep only the prediction index\n",
    "    df = df.drop(y_pred_columns, axis=1)\n",
    "\n",
    "    # Append the processed DataFrame to the list\n",
    "    test_preds.append(df)\n",
    "\n",
    "# Concatenate all individual DataFrames into one big DataFrame\n",
    "test_preds_df = pd.concat(test_preds, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'slide' and 'y_true', and aggregate 'y_pred_max_idx' values into lists\n",
    "df_final = test_preds_df.groupby([\"slide\", \"y_true\"])[\"y_pred_max_idx\"].agg(list).reset_index()\n",
    "\n",
    "# Expand the list of predictions into separate columns (one per prediction)\n",
    "df_final_expanded = df_final[\"y_pred_max_idx\"].apply(pd.Series)\n",
    "\n",
    "# Rename the new columns to indicate their position (e.g., y_pred_max_idx_1, y_pred_max_idx_2, ...)\n",
    "df_final_expanded.columns = [f\"y_pred_max_idx_{i+1}\" for i in df_final_expanded.columns]\n",
    "\n",
    "# Concatenate the original DataFrame (without the list column) with the new expanded prediction columns\n",
    "df_result = pd.concat([df_final.drop(columns=[\"y_pred_max_idx\"]), df_final_expanded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns that contain 'y_pred_max_idx_' (i.e., the expanded prediction columns)\n",
    "colonne_pred = [col for col in df_result.columns if 'y_pred_max_idx_' in col]\n",
    "\n",
    "# Compute the most frequent value (majority vote) across prediction columns for each row\n",
    "df_result['y_pred'] = df_result[colonne_pred].mode(axis=1)[0]  # [0] gets the value (not the Series)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(df_result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_result[[\"slide\", \"y_true\", \"y_pred\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "def extract_metrics(train_true_y, train_pred_y, class_names, model, name_set):\n",
    "    results = []  # List to store metric results\n",
    "    \n",
    "    y_list = train_true_y  # True labels\n",
    "    y_pred = train_pred_y  # Predicted labels\n",
    "\n",
    "    # Compute standard metrics\n",
    "    acc = accuracy_score(y_list, y_pred)\n",
    "    precision = precision_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "    weighted_accuracy = balanced_accuracy_score(y_list, y_pred)\n",
    "    recall = recall_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    # Get the full classification report as a dictionary\n",
    "    report = classification_report(y_list, y_pred, digits=2, target_names=class_names, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Flatten the classification report into a row dictionary\n",
    "    row = {\n",
    "        \"Dataset\": name_set,\n",
    "        \"Fold\": 1,  # ← Temporarily using 1; replace with 'i+1' if inside a loop\n",
    "        \"Accuracy\": acc,\n",
    "        \"Balanced Accuracy\": weighted_accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1_score\": f1\n",
    "    }\n",
    "\n",
    "    # Add per-class metrics to the row\n",
    "    for label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip 'accuracy', which is just a float\n",
    "            for metric_name, value in metrics.items():\n",
    "                row[f\"{label}_{metric_name}\"] = value\n",
    "\n",
    "    results.append(row)  # Append metrics to the results list\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    display(df_results)  # Display the results nicely (only works in Jupyter or IPython)\n",
    "\n",
    "    # Optional: save to Excel file\n",
    "    # df_results.to_excel(model + f\"_{name_set}_5_folds.xlsx\")\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_test[\"y_true\"].to_numpy()\n",
    "y_pred = df_test[\"y_pred\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_metrics(y_true, y_pred, outcome_labels, \"br3\", \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
