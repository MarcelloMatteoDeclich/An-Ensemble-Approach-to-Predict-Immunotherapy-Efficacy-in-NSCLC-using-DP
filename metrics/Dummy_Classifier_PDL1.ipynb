{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"my_library\")\n",
    "from my_library.config import *\n",
    "from my_library.metrics.cmp_metrics import *\n",
    "sys.path.append(\"..\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_df = pd.read_csv(working_directory+\"/projects/I3lung-sqadqc-project/annotations.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utcome = \"PDL1_CATHEGORY\"\n",
    "#model = \"pdl1-3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_dir = f\"{working_directory}/{eval}/train/\" + model\n",
    "\n",
    "train_parquet_paths = []\n",
    "val_parquet_paths = []\n",
    "test_parquet_paths = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # Check if the folder name contains \"attention\"\n",
    "    if \"attention\" in os.path.basename(dirpath).lower():\n",
    "\n",
    "        # If you want to do something with each file inside those folders\n",
    "        for f in filenames:\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\".parquet\"):\n",
    "                #print(f\"   This is a Parquet file: {file_path}\")\n",
    "                train_parquet_paths.append(file_path)\n",
    "train_parquet_paths.sort()\n",
    "\n",
    "#print(\"Sorted Parquet paths:\")\n",
    "#for path in train_parquet_paths:\n",
    "#    print(path)\n",
    "\n",
    "root_dir = f\"{working_directory}/{eval}/val/\" + model\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # Check if the folder name contains \"attention\"\n",
    "    if \"attention\" in os.path.basename(dirpath).lower():\n",
    "\n",
    "        # If you want to do something with each file inside those folders\n",
    "        for f in filenames:\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\".parquet\"):\n",
    "                #print(f\"   This is a Parquet file: {file_path}\")\n",
    "                val_parquet_paths.append(file_path)\n",
    "val_parquet_paths.sort()\n",
    "\n",
    "\n",
    "root_dir = f\"{working_directory}/{eval}/test-int/\" + model\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # Check if the folder name contains \"attention\"\n",
    "    if \"attention\" in os.path.basename(dirpath).lower():\n",
    "\n",
    "        # If you want to do something with each file inside those folders\n",
    "        for f in filenames:\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "            if f.lower().endswith(\".parquet\"):\n",
    "                #print(f\"   This is a Parquet file: {file_path}\")\n",
    "                test_parquet_paths.append(file_path)\n",
    "test_parquet_paths.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_parquet_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parquet_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Dummy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = []\n",
    "numbers = []\n",
    "desired_order = [\"< 1 %\", \"1-49 %\", \">=50 %\"]\n",
    "\n",
    "train_models = []\n",
    "train_classification_reports = []\n",
    "\n",
    "train_true_y=[]\n",
    "train_pred_y =[]\n",
    "\n",
    "for i in train_parquet_paths:\n",
    "    df_predictions = pd.read_parquet(i)\n",
    "    df_joined = pd.merge(df_predictions, csv_df, on ='slide', how = 'left')\n",
    "    #print(df_joined)\n",
    "    distribution = df_joined[[\"slide\", outcome]]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    x_list = np.ones((distribution.shape[0], 1))\n",
    "    y_list = df_joined[outcome].to_numpy()\n",
    "\n",
    "    dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "    dummy_clf.fit(x_list, y_list)\n",
    "\n",
    "    y_pred = dummy_clf.predict(x_list)\n",
    "\n",
    "    train_models.append(dummy_clf)\n",
    "    \n",
    "   \n",
    "    train_true_y.append(y_list)\n",
    "    train_pred_y.append(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "\n",
    "# Assuming your list of dictionaries is stored in the variable `reports`\n",
    "reports = train_classification_reports  # your list of dictionaries\n",
    "\n",
    "def average_report_group(group):\n",
    "    \"\"\"Average a group of report dictionaries.\"\"\"\n",
    "    avg_report = {}\n",
    "    # Loop over each key in the first dictionary of the group.\n",
    "    for key in group[0]:\n",
    "        if key == 'accuracy':\n",
    "            # Average the accuracy values directly.\n",
    "            avg_report[key] = np.mean([r[key] for r in group])\n",
    "        else:\n",
    "            # For keys that are nested dictionaries, average each inner metric.\n",
    "            inner_keys = group[0][key].keys()\n",
    "            avg_inner = {}\n",
    "            for metric in inner_keys:\n",
    "                avg_inner[metric] = np.mean([r[key][metric] for r in group])\n",
    "            avg_report[key] = avg_inner\n",
    "    return avg_report\n",
    "\n",
    "group_size = 5\n",
    "averaged_reports = []\n",
    "\n",
    "# Process the reports list in chunks of 5.\n",
    "for i in range(0, len(reports), group_size):\n",
    "    group = reports[i:i + group_size]\n",
    "    avg = average_report_group(group)\n",
    "    averaged_reports.append(avg)\n",
    "\n",
    "# Now `averaged_reports` contains the averaged dictionary for each group of 5 items.\n",
    "print(averaged_reports)\n",
    "\n",
    "df_averaged_pred = pd.DataFrame(averaged_reports)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = []\n",
    "numbers = []\n",
    "desired_order = [\"< 1 %\", \"1-49 %\", \">=50 %\"]\n",
    "\n",
    "val_models = []\n",
    "val_classification_reports = []\n",
    "\n",
    "val_true_y = []\n",
    "val_pred_y = []\n",
    "\n",
    "for i in val_parquet_paths:\n",
    "    df_predictions = pd.read_parquet(i)\n",
    "    df_joined = pd.merge(df_predictions, csv_df, on ='slide', how = 'left')\n",
    "    #print(df_joined)\n",
    "    distribution = df_joined[[\"slide\", outcome]]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    x_list = np.ones((distribution.shape[0], 1))\n",
    "    y_list = df_joined[outcome].to_numpy()\n",
    "\n",
    "    dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "    dummy_clf.fit(x_list, y_list)\n",
    "\n",
    "    y_pred = dummy_clf.predict(x_list)\n",
    "\n",
    "    val_models.append(dummy_clf)\n",
    "\n",
    "    val_true_y.append(y_list)\n",
    "    val_pred_y.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_parquet_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = []\n",
    "numbers = []\n",
    "desired_order = [\"< 1 %\", \"1-49 %\", \">=50 %\"]\n",
    "\n",
    "test_scores = []\n",
    "test_true_y = []\n",
    "test_pred_y =[]\n",
    "\n",
    "test_list_dataframes = []\n",
    "\n",
    "for i, path in enumerate(test_parquet_paths):\n",
    "    df_predictions = pd.read_parquet(path)\n",
    "    dummy_clf = train_models[i]\n",
    "    df_joined = pd.merge(df_predictions, csv_df, on ='slide', how = 'left')\n",
    "    #print(df_joined)\n",
    "    distribution = df_joined[[\"slide\",outcome]]\n",
    "\n",
    "    display(distribution)\n",
    "\n",
    "    y_true = df_joined[outcome].to_numpy()\n",
    "    print(y_true)\n",
    "    x_list = np.ones((distribution.shape[0], 1))\n",
    "\n",
    "\n",
    "\n",
    "    #print(distribution)\n",
    "    y_pred = dummy_clf.predict(x_list)\n",
    "    print(y_pred)\n",
    "\n",
    "    test_true_y.append(y_true)\n",
    "    test_pred_y.append(y_pred)\n",
    "\n",
    "    distribution[\"y_pred\"] = y_pred\n",
    "    test_list_dataframes.append(distribution)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_list_dataframes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat(test_list_dataframes, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Your list of dictionaries (only a few items are shown for brevity; include all in your actual code)\n",
    "report = train_classification_reports\n",
    "\n",
    "def average_report_group_with_print(group, group_index):\n",
    "    print(f\"\\nProcessing group {group_index} (with {len(group)} items):\")\n",
    "    group_stats = {}\n",
    "    for key in group[0]:\n",
    "        if key == 'accuracy':\n",
    "            values = [r[key] for r in group]\n",
    "            avg_val = np.mean(values)\n",
    "            print(f\"  {key}: values = {values}, average = {avg_val:.4f}\")\n",
    "            group_stats[f\"{key}_avg\"] = avg_val\n",
    "        else:\n",
    "            # For nested dictionary keys (like '1-49 %', '< 1 %', etc.)\n",
    "            for subkey in group[0][key]:\n",
    "                values = [r[key][subkey] for r in group]\n",
    "                avg_val = np.mean(values)\n",
    "                print(f\"  {key} -> {subkey}: values = {values}, average = {avg_val:.4f}\")\n",
    "                flat_key = f\"{key}_{subkey}\"\n",
    "                group_stats[f\"{flat_key}_avg\"] = avg_val\n",
    "    return group_stats\n",
    "\n",
    "group_size = 5\n",
    "averaged_stats = []\n",
    "\n",
    "for i in range(0, len(reports), group_size):\n",
    "    group = reports[i:i+group_size]\n",
    "    group_index = i // group_size\n",
    "    group_stats = average_report_group_with_print(group, group_index)\n",
    "    averaged_stats.append(group_stats)\n",
    "\n",
    "# Create a DataFrame from the averaged statistics.\n",
    "df_train = pd.DataFrame(averaged_stats)\n",
    "\n",
    "print(\"\\nFinal DataFrame with averaged values:\")\n",
    "print(df_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_train=df_train.reset_index()\n",
    "df_val=df_val.reset_index()\n",
    "df_test=df_test.reset_index()\n",
    "''''''\n",
    "df_train.columns = [\"Total number of patients\", \"< 1 %\", \"1-49 %\", \">=50 %\"]\n",
    "df_val .columns = [\"Total number of patients\", \"< 1 %\", \"1-49 %\", \">=50 %\"]\n",
    "df_test.columns = [\"Total number of patients\", \"< 1 %\", \"1-49 %\", \">=50 %\"]'''\n",
    "'''\n",
    "\n",
    "\n",
    "df_train_prefixed = df_train.add_suffix(\"_train\")\n",
    "df_val_prefixed   = df_val.add_suffix(\"_val\")\n",
    "df_test_prefixed  = df_test.add_suffix(\"_test\")\n",
    "\n",
    "df_final = pd.concat([df_train_prefixed, df_val_prefixed, df_test_prefixed], axis=1)\n",
    "print(df_final)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Your list of dictionaries (using your provided sample; in your code include all items)\n",
    "reports = train_classification_reports\n",
    "\n",
    "# Number of items\n",
    "n = len(reports)\n",
    "print(f\"Total number of reports: {n}\\n\")\n",
    "\n",
    "# Function to compute average and CI (CI = 2*std/sqrt(n))\n",
    "def compute_avg_and_ci(values):\n",
    "    avg = np.mean(values)\n",
    "    std = np.std(values, ddof=1) if len(values) > 1 else 0.0\n",
    "    ci = 2 * std / np.sqrt(len(values)) if len(values) > 1 else 0.0\n",
    "    return avg, ci\n",
    "\n",
    "# Dictionary to hold our flattened average and CI values\n",
    "overall_stats = {}\n",
    "\n",
    "# Process each key in the first report as a template.\n",
    "for key in reports[0]:\n",
    "    if key == 'accuracy':\n",
    "        # Collect the accuracy values\n",
    "        values = [r[key] for r in reports]\n",
    "        avg, ci = compute_avg_and_ci(values)\n",
    "        print(f\"Metric '{key}':\")\n",
    "        print(f\"  Values: {values}\")\n",
    "        print(f\"  Average = {avg:.4f}, CI = {ci:.4f}\\n\")\n",
    "        overall_stats[f\"{key}_avg\"] = avg\n",
    "        overall_stats[f\"{key}_ci\"] = ci\n",
    "    else:\n",
    "        # For nested dictionary keys (like '1-49 %', '< 1 %', etc.)\n",
    "        for subkey in reports[0][key]:\n",
    "            values = [r[key][subkey] for r in reports]\n",
    "            avg, ci = compute_avg_and_ci(values)\n",
    "            print(f\"Metric '{key}' -> '{subkey}':\")\n",
    "            print(f\"  Values: {values}\")\n",
    "            print(f\"  Average = {avg:.4f}, CI = {ci:.4f}\\n\")\n",
    "            flat_key = f\"{key}_{subkey}\"\n",
    "            overall_stats[f\"{flat_key}_avg\"] = avg\n",
    "            overall_stats[f\"{flat_key}_ci\"] = ci\n",
    "\n",
    "# Create a DataFrame from the overall statistics dictionary.\n",
    "df = pd.DataFrame([overall_stats])\n",
    "print(\"Final DataFrame with overall averages and CIs:\")\n",
    "display(df)\n",
    "df.to_csv(\"pdl1-dummy-model-train.csv\", index=True)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Your list of dictionaries (using your provided sample; in your code include all items)\n",
    "reports = val_classification_reports\n",
    "\n",
    "# Number of items\n",
    "n = len(reports)\n",
    "print(f\"Total number of reports: {n}\\n\")\n",
    "\n",
    "# Function to compute average and CI (CI = 2*std/sqrt(n))\n",
    "def compute_avg_and_ci(values):\n",
    "    avg = np.mean(values)\n",
    "    std = np.std(values, ddof=1) if len(values) > 1 else 0.0\n",
    "    ci = 2 * std / np.sqrt(len(values)) if len(values) > 1 else 0.0\n",
    "    return avg, ci\n",
    "\n",
    "# Dictionary to hold our flattened average and CI values\n",
    "overall_stats = {}\n",
    "\n",
    "# Process each key in the first report as a template.\n",
    "for key in reports[0]:\n",
    "    if key == 'accuracy':\n",
    "        # Collect the accuracy values\n",
    "        values = [r[key] for r in reports]\n",
    "        avg, ci = compute_avg_and_ci(values)\n",
    "        print(f\"Metric '{key}':\")\n",
    "        print(f\"  Values: {values}\")\n",
    "        print(f\"  Average = {avg:.4f}, CI = {ci:.4f}\\n\")\n",
    "        overall_stats[f\"{key}_avg\"] = avg\n",
    "        overall_stats[f\"{key}_ci\"] = ci\n",
    "    else:\n",
    "        # For nested dictionary keys (like '1-49 %', '< 1 %', etc.)\n",
    "        for subkey in reports[0][key]:\n",
    "            values = [r[key][subkey] for r in reports]\n",
    "            avg, ci = compute_avg_and_ci(values)\n",
    "            print(f\"Metric '{key}' -> '{subkey}':\")\n",
    "            print(f\"  Values: {values}\")\n",
    "            print(f\"  Average = {avg:.4f}, CI = {ci:.4f}\\n\")\n",
    "            flat_key = f\"{key}_{subkey}\"\n",
    "            overall_stats[f\"{flat_key}_avg\"] = avg\n",
    "            overall_stats[f\"{flat_key}_ci\"] = ci\n",
    "\n",
    "# Create a DataFrame from the overall statistics dictionary.\n",
    "df = pd.DataFrame([overall_stats])\n",
    "print(\"Final DataFrame with overall averages and CIs:\")\n",
    "display(df)\n",
    "df.to_csv(\"pdl1-dummy-model-val.csv\", index=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Your list of dictionaries (using your provided sample; in your code include all items)\n",
    "reports = test_scores\n",
    "\n",
    "# Number of items\n",
    "n = len(reports)\n",
    "print(f\"Total number of reports: {n}\\n\")\n",
    "\n",
    "# Function to compute average and CI (CI = 2*std/sqrt(n))\n",
    "def compute_avg_and_ci(values):\n",
    "    avg = np.mean(values)\n",
    "    std = np.std(values, ddof=1) if len(values) > 1 else 0.0\n",
    "    ci = 2 * std / np.sqrt(len(values)) if len(values) > 1 else 0.0\n",
    "    return avg, ci\n",
    "\n",
    "# Dictionary to hold our flattened average and CI values\n",
    "overall_stats = {}\n",
    "\n",
    "# Process each key in the first report as a template.\n",
    "for key in reports[0]:\n",
    "    if key == 'accuracy':\n",
    "        # Collect the accuracy values\n",
    "        values = [r[key] for r in reports]\n",
    "        avg, ci = compute_avg_and_ci(values)\n",
    "        print(f\"Metric '{key}':\")\n",
    "        print(f\"  Values: {values}\")\n",
    "        print(f\"  Average = {avg:.4f}, CI = {ci:.4f}\\n\")\n",
    "        overall_stats[f\"{key}_avg\"] = avg\n",
    "        overall_stats[f\"{key}_ci\"] = ci\n",
    "    else:\n",
    "        # For nested dictionary keys (like '1-49 %', '< 1 %', etc.)\n",
    "        for subkey in reports[0][key]:\n",
    "            values = [r[key][subkey] for r in reports]\n",
    "            avg, ci = compute_avg_and_ci(values)\n",
    "            print(f\"Metric '{key}' -> '{subkey}':\")\n",
    "            print(f\"  Values: {values}\")\n",
    "            print(f\"  Average = {avg:.4f}, CI = {ci:.4f}\\n\")\n",
    "            flat_key = f\"{key}_{subkey}\"\n",
    "            overall_stats[f\"{flat_key}_avg\"] = avg\n",
    "            overall_stats[f\"{flat_key}_ci\"] = ci\n",
    "\n",
    "# Create a DataFrame from the overall statistics dictionary.\n",
    "df = pd.DataFrame([overall_stats])\n",
    "print(\"Final DataFrame with overall averages and CIs:\")\n",
    "display(df)\n",
    "df.to_csv(\"pdl1-dummy-model-test.csv\", index=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''display(df_final)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_final.to_csv(\"pdl1-dummy-model.csv\", index=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_5_elements (train_true_y, train_pred_y):\n",
    "    liste =train_true_y\n",
    "    y_trues = []\n",
    "    for i in range(0, len(liste), 5):\n",
    "        unione = [item for sublist in liste[i:i+5] for item in sublist]\n",
    "        y_trues.append(unione)\n",
    "        #print(unione)\n",
    "\n",
    "    liste = train_pred_y\n",
    "    y_preds = []\n",
    "    for i in range(0, len(liste), 5):\n",
    "        unione = [item for sublist in liste[i:i+5] for item in sublist]\n",
    "        y_preds.append(unione)\n",
    "        #print(unione)\n",
    "    return y_trues, y_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_trues, y_preds = extract_5_elements(train_true_y, train_pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(df_input, name): \n",
    "    df = df_input.copy()\n",
    "    # Removing \"Fold\" column\n",
    "    df = df.drop(columns=[\"Dataset\",\"Fold\"])  \n",
    "    mean_values = df.mean()\n",
    "    std_values = df.std()\n",
    "    n = len(df)\n",
    "    ci_values = (2 * std_values) / np.sqrt(n)\n",
    "    result_df = pd.DataFrame({\n",
    "    \"Column\": mean_values.index,\n",
    "    \"Mean\": mean_values.values,\n",
    "    \"CI\": ci_values.values\n",
    "    })\n",
    "    result_df = result_df.add_prefix(name)\n",
    "    display(result_df)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(train_true_y, train_pred_y, model, name_set):\n",
    "    y_trues, y_preds = extract_5_elements(train_true_y, train_pred_y)\n",
    "    results = []  # Store results\n",
    "\n",
    "    for i in range(len(y_trues)):\n",
    "        y_list = y_trues[i]\n",
    "        y_pred = y_preds[i]\n",
    "        acc = accuracy_score(y_list, y_pred)\n",
    "        precision = precision_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "        recall = recall_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "        f1 = f1_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        report = classification_report(y_list, y_pred, digits=2, output_dict=True, zero_division=0)\n",
    "\n",
    "        # Flatten report into a list\n",
    "        row = {\"Dataset\": name_set, \"Fold\": i+1, \"Accuracy\": acc, \"Precision\" : precision, \"Recall\": recall, \"F1_score\" : f1}\n",
    "\n",
    "        for label, metrics in report.items():\n",
    "            if isinstance(metrics, dict):  # Avoid 'accuracy' which is a single value\n",
    "                for metric_name, value in metrics.items():\n",
    "                    row[f\"{label}_{metric_name}\"] = value\n",
    "        \n",
    "        results.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "    display(df_results)\n",
    "    #df_results.to_excel(model+f\"_{name_set}_5_folds.xlsx\")\n",
    "\n",
    "    avg = average(df_results, name_set)\n",
    "\n",
    "    return df_results, avg\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i in range (5):\n",
    "    print(len(y_trues[i]))\n",
    "\n",
    "for i in range (5):\n",
    "    print(len(y_preds[i]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, avg_1 = extract_metrics( train_true_y, train_pred_y, model, \"train\")\n",
    "df2, avg_2= extract_metrics( val_true_y, val_pred_y, model, \"val\")\n",
    "df3, avg_3 = extract_metrics( test_true_y, test_pred_y, model, \"test\")\n",
    "\n",
    "df_final = pd.concat([df1, df2, df3], axis=0, ignore_index=True)\n",
    "#df_final.to_excel(model+f\"_complete_5_folds.xlsx\")\n",
    "\n",
    "#df_final_avg = pd.concat([avg_1, avg_2, avg_3], axis=1, ignore_index=False)\n",
    "#df_final_avg.to_excel(model+f\"_complete_averages_5_folds.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =test_df.rename(columns ={outcome: \"y_true\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppiamo i dati per slide e y_true e creiamo nuove colonne per ogni valore di y_pred_max_idx\n",
    "df_final = test_df.groupby([\"slide\", \"y_true\"])[\"y_pred\"].agg(list).reset_index()\n",
    "\n",
    "# Espandiamo la lista in colonne separate\n",
    "df_final_expanded = df_final[\"y_pred\"].apply(pd.Series)\n",
    "\n",
    "# Rinominiamo le nuove colonne\n",
    "df_final_expanded.columns = [f\"pred_{i+1}\" for i in df_final_expanded.columns]\n",
    "\n",
    "# Concatenazione con le colonne originali\n",
    "df_result = pd.concat([df_final.drop(columns=[\"y_pred\"]), df_final_expanded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [col for col in df_result.columns if 'pred_' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supponiamo che il tuo DataFrame si chiami df\n",
    "colonne_pred = [col for col in df_result.columns if 'pred_' in col]#[f\"pred_{i}\" for i in range(1, 6)]\n",
    "\n",
    "# Calcola il valore pi√π frequente (majority vote) per ogni riga\n",
    "df_result['y_pred'] = df_result[colonne_pred].mode(axis=1)[0]\n",
    "\n",
    "# Visualizza il DataFrame aggiornato\n",
    "print(df_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_result[[\"slide\", \"y_true\", \"y_pred\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "def extract_metrics(train_true_y, train_pred_y, model, name_set):\n",
    "    \n",
    "    results = []  # Store results\n",
    "    y_list = train_true_y\n",
    "    y_pred = train_pred_y\n",
    "    acc = accuracy_score(y_list, y_pred)\n",
    "\n",
    "    precision = precision_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "    weighted_accuracy = balanced_accuracy_score(y_list, y_pred)\n",
    "\n",
    "    recall = recall_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(y_list, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    report = classification_report(y_list, y_pred, digits=2,   output_dict=True, zero_division=0)\n",
    "\n",
    "    # Flatten report into a list\n",
    "    row = {\"Dataset\": name_set, \"Fold\": i+1, \"Accuracy\": acc, \"Balanced Accuracy\":weighted_accuracy, \"Precision\" : precision, \"Recall\": recall, \"F1_score\" : f1}\n",
    "\n",
    "    for label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Avoid 'accuracy' which is a single value\n",
    "            for metric_name, value in metrics.items():\n",
    "                row[f\"{label}_{metric_name}\"] = value\n",
    "        \n",
    "    results.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "    display(df_results)\n",
    "    #df_results.to_excel(model+f\"_{name_set}_5_folds.xlsx\")\n",
    "\n",
    "    \n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_test[\"y_true\"].to_numpy()\n",
    "y_pred = df_test[\"y_pred\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_metrics(y_true, y_pred, \"br3\", \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
