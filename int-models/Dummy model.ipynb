{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Configurazione\n",
    "subfolder = \"/media/mmd/Extreme SSD/work/bags/uni/no-qc/10x reinhard_fast\"  # Cambia con il nome della sottocartella\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Configurazione\n",
    "\n",
    "csv_file = \"/media/mmd/Samsung_T5/GitHub/UMD/projects/I3lung-sqadqc-project/annotations.csv\"  # File CSV con le label\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Carica il DataFrame con le label\n",
    "df = pd.read_csv(csv_file)  # Assumendo che abbia colonne: 'filename' e 'label'\n",
    "df = df.dropna(subset=[\"PDL1_CATHEGORY\"])\n",
    "df.set_index(\"slide\", inplace=True)\n",
    "\n",
    "# Carica tutti i file .pt\n",
    "file_paths = [os.path.join(subfolder, f) for f in os.listdir(subfolder) if f.endswith(\".pt\")]\n",
    "\n",
    "# Contenitori per le feature e le label\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for file in file_paths:\n",
    "    filename = os.path.basename(file).replace(\".pt\",\"\")  # Estrai solo il nome del file\n",
    "    print(filename)\n",
    "    # Se il file non ha una label, viene saltato\n",
    "    if filename not in df.index:\n",
    "        #print(f\"⚠️ {filename} non ha una label nel CSV, saltato.\")\n",
    "        continue  \n",
    "\n",
    "    label = df.loc[filename, \"PDL1_CATHEGORY\"]  # Trova la label nel DataFrame\n",
    "\n",
    "    # Se la label è NaN o None, salta il file\n",
    "    if pd.isna(label):\n",
    "        print(f\"⚠️ {filename} ha una label mancante, saltato.\")\n",
    "        continue  \n",
    "\n",
    "    data = torch.load(file, map_location=device)  # Carica il tensore\n",
    "    print(data.shape)\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        X_list.append(data.cpu().numpy())  # Sposta su CPU e converte in NumPy\n",
    "        y_list.append(label)  # Assegna la label corretta\n",
    "    else:\n",
    "        print(f\"⚠️ Formato non supportato in {filename}, atteso un torch.Tensor.\")\n",
    "\n",
    "# Controllo se ci sono abbastanza dati\n",
    "if len(X_list) == 0:\n",
    "    raise ValueError(\"❌ Nessun dato valido trovato. Controlla i file e le label nel CSV.\")\n",
    "\n",
    "# Converti in array NumPy\n",
    "\n",
    "print(X_list[0].shape)\n",
    "print(y_list[0])\n",
    "\n",
    "\n",
    "# Calcolare la media di ogni array nella lista\n",
    "X_aggregated = np.array([X.mean(axis=0) for X in X_list])\n",
    "\n",
    "# Ora possiamo eseguire vstack\n",
    "X_final = np.vstack(X_aggregated)\n",
    "\n",
    "# Mostriamo il risultato\n",
    "print(\"Dimensione di X_final:\", X_final.shape)\n",
    "print(X_final)\n",
    "\n",
    "\n",
    "#X = np.vstack(X_list)\n",
    "X = X_final\n",
    "y = np.array(y_list)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Suddivisione train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dummy Classifier\n",
    "dummy_clf = DummyClassifier(strategy=\"uniform\", random_state=42)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Predizioni\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "# Accuratezza generale\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Report dettagliato per classe\n",
    "report = classification_report(y_test, y_pred, digits=2, output_dict=True)\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"Accuratezza del Dummy Classifier: {acc:.2f}\\n\")\n",
    "print(\"Statistiche per classe:\\n\")\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predizioni\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "# Accuratezza totale\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Report dettagliato per classe\n",
    "report = classification_report(y_test, y_pred, digits=2, output_dict=True)\n",
    "\n",
    "# Estrazione dell'accuratezza per classe\n",
    "class_accuracies = {f\"Classe {label}\": report[str(label)][\"recall\"] for label in np.unique(y_test)}\n",
    "\n",
    "# Matrice di confusione\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"Accuratezza del Dummy Classifier: {acc:.2f}\\n\")\n",
    "print(\"Accuratezza per classe:\")\n",
    "for label, class_acc in class_accuracies.items():\n",
    "    print(f\"{label}: {class_acc:.2f}\")\n",
    "\n",
    "# Visualizzazione della matrice di confusione\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.xlabel(\"Predetto\")\n",
    "plt.ylabel(\"Reale\")\n",
    "plt.title(\"Matrice di Confusione\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Estrazione dell'accuratezza per classe\n",
    "class_accuracies = {f\"Classe {label}\": report[str(label)][\"recall\"] for label in np.unique(y_test)}\n",
    "\n",
    "# Matrice di confusione\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"Accuratezza del Dummy Classifier: {acc:.2f}\\n\")\n",
    "print(\"Accuratezza per classe:\")\n",
    "for label, class_acc in class_accuracies.items():\n",
    "    print(f\"{label}: {class_acc:.2f}\")\n",
    "\n",
    "# Visualizzazione della matrice di confusione\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.xlabel(\"Predetto\")\n",
    "plt.ylabel(\"Reale\")\n",
    "plt.title(\"Matrice di Confusione\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"my_library\")\n",
    "from my_library.config import *\n",
    "from my_library.metrics.cmp_metrics import *\n",
    "sys.path.append(\"..\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_y = y_test\n",
    "predicted_classes = y_pred\n",
    "\n",
    "\n",
    "# Compute standard metrics\n",
    "accuracy = accuracy_score(true_y, predicted_classes)\n",
    "weighted_accuracy = balanced_accuracy_score(true_y, predicted_classes)\n",
    "precision = precision_score(true_y, predicted_classes, average=\"weighted\", zero_division=0)\n",
    "recall = recall_score(true_y, predicted_classes, average=\"weighted\", zero_division=0)\n",
    "f1 = f1_score(true_y, predicted_classes, average=\"weighted\", zero_division=0)\n",
    "#conf_matrix = multilabel_confusion_matrix(true_y, predicted_classes, labels=range(3))\n",
    "\n",
    "# Compute per-class accuracy\n",
    "#class_accuracies = {}\n",
    "#for cls in range(3):\n",
    "  #  cls_indices = (true_y == cls)\n",
    "   # cls_accuracy = accuracy_score(true_y[cls_indices], predicted_classes[cls_indices]) if cls_indices.any() else 0.0\n",
    "  #  class_accuracies[f\"Accuracy_Class_{cls}\"] = cls_accuracy\n",
    "\n",
    "# Return metrics as a dictionary\n",
    "abc = {\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Weighted_Accuracy\":weighted_accuracy,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1-Score\": f1,\n",
    "    #\"Confusion Matrix\": conf_matrix.tolist(),\n",
    "    #\"Class Accuracies\": class_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(parent_directory, n_classes, base_folder):\n",
    "    parent_directory = base_folder\n",
    "\n",
    "    out_lab, out = find_classes(base_folder)\n",
    "    display(out_lab)\n",
    "    print(f\"This is outlab {len(out_lab)}\")\n",
    "    print(out)\n",
    "    n_classes = len(out_lab)\n",
    "    n_c = len(out_lab)\n",
    "\n",
    "    averaged_predictions = load_and_average_predictions(parent_directory, n_c)\n",
    "    from pprint import pprint\n",
    "    import json\n",
    "\n",
    "    #metrics = compute_metrics_with_class_accuracy(averaged_predictions, n_classes)\n",
    "    metrics = compute_metrics_with_class_accuracy(averaged_predictions, n_c)\n",
    "\n",
    "\n",
    "    display(metrics)\n",
    "    \n",
    "    for metric, value in metrics.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(metric + \":\")\n",
    "            for sub_metric, sub_value in value.items():\n",
    "                print(f\"  {sub_metric}: {sub_value}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    class_values = out_lab.values()\n",
    "    class_names = [f\"Class {i}\" for i in class_values]\n",
    "\n",
    "    #class_names = [f\"Class {i}\" for i in range(n_classes)]\n",
    "    \n",
    "    conf_matrix = np.array(metrics[\"Confusion Matrix\"])\n",
    "    plot_confusion_matrix(conf_matrix, class_names, \"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
